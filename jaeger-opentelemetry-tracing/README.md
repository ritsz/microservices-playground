## Tracing Opentelemetry with Jaeger

Jaeger accepts traces generated by Opentelemetry as well as Zipkin. This example shows how Jaeger consumes Opentelemetry traces.
### Envoy Proxy
* A `frontend-proxy` services servers as the reverse proxy of this setup.
* The proxy is running using Envoy.
* `envoy-proxy.yaml` file has the configuration for Envoy.
* The `virtual_hosts` and `cluster` configuration for `frontend-proxy` is the same as `envoy-proxy` example.
* `generate_request_id` property of the `HttpConnectionManager` is set. The `HttpConnectionManager` will generate `x-request-id` header if it is not present.
* To perform tracing of request, we configure `Opentelemetry` tracers called `envoy.tracers.dynamic_ot`.
* The tracer is configured using `DynamicOtConfig`.
  * `localAgentHostPort`: The location of the agent where envoy will publish it's Opentelemetry traces.
* The `response_headers_to_add` config adds the headers specified in the config to the response to the client. In this example, we add request ID and `uber-trace-id`.
  ```bash
  $ curl -vsA "Testing" http://127.0.0.1:8080/ratings |  jq .
  *   Trying 127.0.0.1...
  * TCP_NODELAY set
  * Connected to 127.0.0.1 (127.0.0.1) port 8080 (#0)
  > GET /ratings HTTP/1.1
  > Host: 127.0.0.1:8080
  > User-Agent: Testing
  > Accept: */*
  >
  < HTTP/1.1 200 OK
  < content-type: text/html; charset=utf-8
  < content-length: 80
  < server: envoy
  < date: Tue, 25 Jan 2022 18:19:36 GMT
  < x-envoy-upstream-service-time: 2
  < uber-trace-id: 595c59216c1438d3:595c59216c1438d3:0:1
  < x-request-id: 6b5277b2-ca93-98bf-9140-ab295b63e98e
  <
  { [80 bytes data]
  * Connection #0 to host 127.0.0.1 left intact
  * Closing connection 0
  [
    {
      "id": "f22873bb-56b6-47fd-8980-9b9dc9d20074",
      "name": "Anand",
      "rating": "5"
    }
  ]
  ```

### Jaeger
* The Jaeger service is running in the kubernetes cluster.
* `jaeger` service is **NOT** started with the config `COLLECTOR_ZIPKIN_HOST_PORT=9411`, since jaeger is not collecting Zipkin traces.
* `frontend-proxy`, `film-service` and `rating-service` are configured such that they forward `POST` their trace/span information to this `jaeger` service.
* The `DynamicOtConfig` in Envoy config results in additional headers being added to the requests that reach the other services.
  ```bash
  $ curl -X POST http://127.0.0.1:8080/films --data "{       
    'name': 'Titanic',
    'language': 'English'
  }" | jq .
  [
      {
        "name": "Mad Max",
        "language": "English",
        "id": "b1757edc-454d-47b1-ab82-c18c06b6002b"
      },
      {
        "name": "Titanic",
        "language": "English",
        "id": "eeb67863-187b-4016-a190-db48ec568fa4"
      }
  ]
  ```
  ```bash
  ### Logs
  film-service-1    |  (service 'film-service'):(hostname dc7edac32834) Headers: {
    'X-Ot-Span-Context': None,
    'X-Request-Id': '3e7309cf-904f-9c74-9b66-fb571ef9e843',
    'X-B3-TraceId': None,
    'X-B3-SpanId': None,
    'X-B3-ParentSpanId': None,
    'X-B3-Sampled': None,
    'X-B3-Flags': None,
    'uber-trace-id': 'fc40ac68893c0dad:fc40ac68893c0dad:0:1',
    'sw8': None}
  ```
* A tracer is initialized, that is automatically traces all requests to the flask app.
  ```python
    def init_tracer():
        logging.getLogger('').handlers = []
        logging.basicConfig(format='%(message)s', level=logging.DEBUG)
        config = Config(
            config={  # usually read from some yaml config
                'sampler': {
                    'type': 'const',
                    'param': 1,
                },
                'local_agent': {
                    'reporting_host': "jaeger",
                    'reporting_port': 6831,
                },
                'logging': True,
            },
            service_name=SERVICE_NAME,
        )
        return config.initialize_tracer()

    tracer = FlaskTracing(init_tracer, True, app)
  ```
  ![Jaeger dashboard](./jaeger.png)

### Services
* 2 services are deployed:
  * `film-service`:
    * holds the record of the id, name, and language of a film.
    * Port: `5000`
  * `rating-service`:
    * holds the record of the id, name, and rating of a film.
    * All ratings are 5 star right now.
    * Port: `6000`
* Both the services are created using flask.

### Deployment
#### Build Images
* Each service and envoy-proxy has a `Dockerfile`.
* The `Dockerfile` is used to create the image of the service.
#### Deploy on Kubernetes
* `kubernetes-deployment.yaml` file is used to deploy all the components.
* `film-service`, `rating-service` and `frontend-proxy` have corresponding `Deployment` and `Service`.
* Each service is discoverable using their `Service` names.
* `Service` map to `Deployment` using label selectors.
* Each pod has one container. The image for the container need to be build using the `Dockerfile`.
* Each container also needs to expose the ports that they want other pods/services to connect to.
  * For example `film-service` container needs to expose `5000` port so that other services can connect to this container's application.
  * **Note that this exposes the ports only inside the kubernetes network to other pods/services and NOT to the host network or internet!**
    ```bash
    $ kubectl apply -f kubernetes-deployment.yaml
    service/frontend-proxy created
    service/film-service created
    service/rating-service created
    service/jaeger created
    deployment.apps/frontend-proxy-deployment created
    deployment.apps/film-service-deployment created
    deployment.apps/rating-service-deployment created
    deployment.apps/jaeger-deployment created

    $ kubectl port-forward svc/frontend-proxy 8080:8080 --address 0.0.0.0
    Forwarding from 0.0.0.0:8080 -> 8080

    $ curl -X POST http://127.0.0.1:8080/films --data "{
      'name': 'Anand',
      'language': 'English'
    }"
    [{"name": "Anand", "language": "English", "id": "42aa33ed-4dbf-4694-a14d-a9d84eeb2cbd"}]

    $ curl -X GET http://127.0.0.1:8080/ratings | jq .
    [
      {
        "id": "42aa33ed-4dbf-4694-a14d-a9d84eeb2cbd",
        "name": "Anand",
        "rating": "5"
      }
    ]
    ```

### Test on Kubernetes
* Build the images.
* Deploys the services and the envoy proxy.
* Runs `curl` commands to check the APIs.
* Scales the services.
* Runs the `curl` commands to check the APIs again.
  * The scaled services don't share a common data fabric layer, they hold their data in memory.
  * `POST` could go to one replica and `GET` to another.
  * In the scaled setup, `POST` + `GET` verification can fail.
```
./k8s-verify.sh
```
